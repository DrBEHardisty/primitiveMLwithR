## The goal of this project is to predict the UT Stage Fibrosis Score using machine learning;
# and remembering that ML on small datasets is really just coding practice.
# The project uses simulated data that is drawn from the likely distributions
# (and, sometimes, the truncated distributions) of the original dataset,
# which belongs to the Univeristy of Utah School of Medicine.

## Load Worthington's "ipak" function:
ipak <- function(pkg){
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg))
    install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE)
}

## Give "packages" the names of the packages you'll use in your data tasks:
packages <- c("tidyverse","plyr","dplyr","magrittr","skimr","mlr","e1071","ggplot2","OneR","caret","janitor","msm")

## Run "ipak" and open all the packages:
ipak(packages)

## Read in the data.
  ## Put all of the simualted datasets into their own folder.
  ## Set the wd to that folder.
  ## Now use "lapply" to open them all:
dataFiles <- lapply(Sys.glob("fake*.csv"), read.csv)

## Take a fast look at the data.
# Don't forget to make sure dimensions of all are the same.

## Check that each column of each dataset is typed correctly:
str(dataFiles[[1]])
str(dataFiles[[2]])
str(dataFiles[[3]])
str(dataFiles[[4]])
str(dataFiles[[5]])

#Classes ‘tbl_df’, ‘tbl’ and 'data.frame':	219 obs. of  19 variables:

## Use R package "janitor" to "clean" column names.
# (Some ML packages get glitchy over R column names).
dataFiles[[1]] <- dataFiles[[1]] %>% clean_names()
dataFiles[[2]] <- dataFiles[[2]] %>% clean_names()
dataFiles[[3]] <- dataFiles[[3]] %>% clean_names()
dataFiles[[4]] <- dataFiles[[4]] %>% clean_names()
dataFiles[[5]] <- dataFiles[[5]] %>% clean_names()

## If these give different names, some columns need to be renamed for ease of handling by R package "caret".
names(dataFiles[[1]])
make.names(names(dataFiles[[1]]))

names(dataFiles[[2]])
make.names(names(dataFiles[[2]]))

names(dataFiles[[3]])
make.names(names(dataFiles[[3]]))

names(dataFiles[[4]])
make.names(names(dataFiles[[4]]))

names(dataFiles[[5]])
make.names(names(dataFiles[[5]]))

## Make sure factor variables are factors for ease of model fitting.
dataFiles[[1]]$sim_diab <- as.factor(dataFiles[[1]]$sim_diab)
dataFiles[[2]]$sim_diab <- as.factor(dataFiles[[2]]$sim_diab)
dataFiles[[3]]$sim_diab <- as.factor(dataFiles[[3]]$sim_diab)
dataFiles[[4]]$sim_diab <- as.factor(dataFiles[[4]]$sim_diab)
dataFiles[[5]]$sim_diab <- as.factor(dataFiles[[5]]$sim_diab)

dataFiles[[1]]$sim_hypertension <- as.factor(dataFiles[[1]]$sim_hypertension)
dataFiles[[2]]$sim_hypertension <- as.factor(dataFiles[[2]]$sim_hypertension)
dataFiles[[3]]$sim_hypertension <- as.factor(dataFiles[[3]]$sim_hypertension)
dataFiles[[4]]$sim_hypertension <- as.factor(dataFiles[[4]]$sim_hypertension)
dataFiles[[5]]$sim_hypertension <- as.factor(dataFiles[[5]]$sim_hypertension)

dataFiles[[1]]$sim_sleep_ap <- as.factor(dataFiles[[1]]$sim_sleep_ap)
dataFiles[[2]]$sim_sleep_ap <- as.factor(dataFiles[[2]]$sim_sleep_ap)
dataFiles[[3]]$sim_sleep_ap <- as.factor(dataFiles[[3]]$sim_sleep_ap)
dataFiles[[4]]$sim_sleep_ap <- as.factor(dataFiles[[4]]$sim_sleep_ap)
dataFiles[[5]]$sim_sleep_ap <- as.factor(dataFiles[[5]]$sim_sleep_ap)

dataFiles[[1]]$sim_stroke <- as.factor(dataFiles[[1]]$sim_stroke)
dataFiles[[2]]$sim_stroke <- as.factor(dataFiles[[2]]$sim_stroke)
dataFiles[[3]]$sim_stroke <- as.factor(dataFiles[[3]]$sim_stroke)
dataFiles[[4]]$sim_stroke <- as.factor(dataFiles[[4]]$sim_stroke)
dataFiles[[5]]$sim_stroke <- as.factor(dataFiles[[5]]$sim_stroke)

dataFiles[[1]]$sim_ablate <- as.factor(dataFiles[[1]]$sim_ablate)
dataFiles[[2]]$sim_ablate <- as.factor(dataFiles[[2]]$sim_ablate)
dataFiles[[3]]$sim_ablate <- as.factor(dataFiles[[3]]$sim_ablate)
dataFiles[[4]]$sim_ablate <- as.factor(dataFiles[[4]]$sim_ablate)
dataFiles[[5]]$sim_ablate <- as.factor(dataFiles[[5]]$sim_ablate)

dataFiles[[1]]$utah_score <- as.factor(dataFiles[[1]]$utah_score)
dataFiles[[2]]$utah_score <- as.factor(dataFiles[[2]]$utah_score)
dataFiles[[3]]$utah_score <- as.factor(dataFiles[[3]]$utah_score)
dataFiles[[4]]$utah_score <- as.factor(dataFiles[[4]]$utah_score)
dataFiles[[5]]$utah_score <- as.factor(dataFiles[[5]]$utah_score)

## Inspect the transformations.
str(dataFiles[[1]])
str(dataFiles[[2]])
str(dataFiles[[3]])
str(dataFiles[[4]])
str(dataFiles[[5]])

# R package "skimr" will show you what you're working with.
# Because all 5 were drawn based on parameters and distributions from the real data,
# we expect each of the simlated datasets to have *very similar* eveything.
skim(dataFiles[[1]])
skim(dataFiles[[2]])
skim(dataFiles[[3]])
skim(dataFiles[[4]])
skim(dataFiles[[5]])

## Now remove the variable "x", an ID generated by the sampling procedures,
# before doing anything else.
dataFiles[[1]] <- dataFiles[[1]] %>% dplyr::select(-x)
dataFiles[[2]] <- dataFiles[[2]] %>% dplyr::select(-x)
dataFiles[[3]] <- dataFiles[[3]] %>% dplyr::select(-x)
dataFiles[[4]] <- dataFiles[[4]] %>% dplyr::select(-x)
dataFiles[[5]] <- dataFiles[[5]] %>% dplyr::select(-x)

## Inspect the output of the deletions.
names(dataFiles[[1]])
names(dataFiles[[2]])
names(dataFiles[[3]])
names(dataFiles[[4]])
names(dataFiles[[5]])

## Notice that UT Stage Fibrosis Score is very poorly represented
# in our full dataset (as in the actual real data!).
# There are only 56/500 "3" in the fake_ra_data_i that I'm using:
table(dataFiles[[1]]$utah_score)

## We can use R package "UBL" to generate, from the fake data,
# another fake data set with balanced target classes:
library(UBL)

## The "adaysn" algorithm, "Adaptive SYNthetic sampling",
# generates a new data based on random draws for kNN (here set to k = 5)
# and constructs a dataset where our classification problem (utah_score) can be solved.
# Beta = 1 means we want a 100% balanced dataset produced.
# Dist = distrubtion used to computed distances.
new_fake_fake_ra_data_1 <- AdasynClassif(utah_score~., dataFiles[[1]], beta = 1, dth = 0.95,
                                         k = 5, dist = "HVDM")

new_fake_fake_ra_data_2 <- AdasynClassif(utah_score~., dataFiles[[2]], beta = 1, dth = 0.95,
                                         k = 5, dist = "HVDM")

new_fake_fake_ra_data_3 <- AdasynClassif(utah_score~., dataFiles[[3]], beta = 1, dth = 0.95,
                                         k = 5, dist = "HVDM")

new_fake_fake_ra_data_4 <- AdasynClassif(utah_score~., dataFiles[[4]], beta = 1, dth = 0.95,
                                         k = 5, dist = "HVDM")

new_fake_fake_ra_data_5 <- AdasynClassif(utah_score~., dataFiles[[5]], beta = 1, dth = 0.95,
                                         k = 5, dist = "HVDM")

## Take another look at the old class distributions of the target, "utah_score"
summary(dataFiles[[1]]$utah_score)
summary(dataFiles[[2]]$utah_score)
summary(dataFiles[[3]]$utah_score)
summary(dataFiles[[4]]$utah_score)
summary(dataFiles[[5]]$utah_score)

## versus the new class distributions based on repeated under-sampling of classes 0 and 1.:
summary(new_fake_fake_ra_data_1$utah_score)
summary(new_fake_fake_ra_data_2$utah_score)
summary(new_fake_fake_ra_data_3$utah_score)
summary(new_fake_fake_ra_data_4$utah_score)
summary(new_fake_fake_ra_data_5$utah_score)
# With even distributions of each possible class of the "utah_score"
# we should get better prediction/classification results from our data.

## Now we're ready to make training and test datasets
# from the simulated simulated datasets.
# Personally, I like ye ol' 80/20 split.
set.seed(1234)

training.sample.1 <- new_fake_fake_ra_data_1$utah_score %>% 
  createDataPartition(p = 0.80, list = FALSE)
train.data.1  <- new_fake_fake_ra_data_1[training.sample.1, ]
test.data.1 <- new_fake_fake_ra_data_1[-training.sample.1, ]

set.seed(1234)

training.sample.2 <- new_fake_fake_ra_data_2$utah_score %>% 
  createDataPartition(p = 0.80, list = FALSE)
train.data.2  <- new_fake_fake_ra_data_2[training.sample.2, ]
test.data.2 <- new_fake_fake_ra_data_2[-training.sample.2, ]

set.seed(1234)

training.sample.3 <- new_fake_fake_ra_data_3$utah_score %>% 
  createDataPartition(p = 0.80, list = FALSE)
train.data.3  <- new_fake_fake_ra_data_3[training.sample.3, ]
test.data.3 <- new_fake_fake_ra_data_3[-training.sample.3, ]

set.seed(1234)

training.sample.4 <- new_fake_fake_ra_data_4$utah_score %>% 
  createDataPartition(p = 0.80, list = FALSE)
train.data.4 <- new_fake_fake_ra_data_4[training.sample.4, ]
test.data.4 <- new_fake_fake_ra_data_4[-training.sample.4, ]

set.seed(1234)

training.sample.5 <- new_fake_fake_ra_data_5$utah_score %>% 
  createDataPartition(p = 0.80, list = FALSE)
train.data.5 <- new_fake_fake_ra_data_5[training.sample.5, ]
test.data.5 <- new_fake_fake_ra_data_5[-training.sample.5, ]

# View training and test datasets:
head(train.data.1)
head(test.data.1)

head(train.data.2)
head(test.data.2)

head(train.data.3)
head(test.data.3)

head(train.data.4)
head(test.data.4)

head(train.data.5)
head(test.data.5)

## Set seed and training control parameters for the model fittings:
library(DMwR)
library(ROSE)

set.seed(12345)

control <- caret::trainControl(method="cv", number=10, classProbs= TRUE,summaryFunction = defaultSummary,
                               selectionFunction = "best")

## Choose an accuracy metric. Option: "Accuracy", "logLoss", "ROC" and "Kappa."
metric <- "Accuracy"

## Give levels of the target variable names so "caret" doesn't get confused:
levels(train.data.1$utah_score) <- c("first_class", "second_class","third_class")
levels(test.data.1$utah_score) <- c("first_class", "second_class","third_class")

levels(train.data.2$utah_score) <- c("first_class", "second_class","third_class")
levels(test.data.2$utah_score) <- c("first_class", "second_class","third_class")

levels(train.data.3$utah_score) <- c("first_class", "second_class","third_class")
levels(test.data.3$utah_score) <- c("first_class", "second_class","third_class")

levels(train.data.4$utah_score) <- c("first_class", "second_class","third_class")
levels(test.data.4$utah_score) <- c("first_class", "second_class","third_class")

levels(train.data.5$utah_score) <- c("first_class", "second_class","third_class")
levels(test.data.5$utah_score) <- c("first_class", "second_class","third_class")


## k-Nearest Neighbors 1-5.
library(kknn)
library(MLmetrics)

set.seed(12345)

m_kknn_1 <- caret::train(utah_score~., data=train.data.1, method="kknn", metric=metric, 
                       trControl=control, preProcess = c("center", "scale"),na.action=na.omit)

m_kknn_2 <- caret::train(utah_score~., data=train.data.2, method="kknn", metric=metric, 
                       trControl=control, preProcess = c("center", "scale"),na.action=na.omit)

m_kknn_3 <- caret::train(utah_score~., data=train.data.3, method="kknn", metric=metric, 
                       trControl=control, preProcess = c("center", "scale"),na.action=na.omit)

m_kknn_4 <- caret::train(utah_score~., data=train.data.4, method="kknn", metric=metric, 
                       trControl=control, preProcess = c("center", "scale"),na.action=na.omit)

m_kknn_5 <- caret::train(utah_score~., data=train.data.5, method="kknn", metric=metric, 
                       trControl=control, preProcess = c("center", "scale"),na.action=na.omit)
print(m_kknn_1)
print(m_kknn_2)
print(m_kknn_3)
print(m_kknn_4)
print(m_kknn_5)

ggplot(m_kknn_1)
ggplot(m_kknn_2)
ggplot(m_kknn_3)
ggplot(m_kknn_4)
ggplot(m_kknn_5)

## Neural networks 1-5.
set.seed(12345)

m_nnet_1 <- caret::train(utah_score~., data=train.data.1, method="nnet", metric=metric,
                       trControl=control, preProcess = c("center", "scale"), trace = TRUE, na.action = na.pass, maxit = 100000)

m_nnet_2 <- caret::train(utah_score~., data=train.data.2, method="nnet", metric=metric,
                       trControl=control, preProcess = c("center", "scale"), trace = TRUE, na.action = na.pass,maxit = 100000)

m_nnet_3 <- caret::train(utah_score~., data=train.data.3, method="nnet", metric=metric,
                       trControl=control, preProcess = c("center", "scale"), trace = TRUE, na.action = na.pass,maxit = 100000)

m_nnet_4 <- caret::train(utah_score~., data=train.data.4, method="nnet", metric=metric,
                       trControl=control, preProcess = c("center", "scale"), trace = TRUE, na.action = na.pass,maxit = 100000)

m_nnet_5 <- caret::train(utah_score~., data=train.data.5, method="nnet", metric=metric,
                       trControl=control, preProcess = c("center", "scale"), trace = TRUE, na.action = na.pass,maxit = 100000)
print(m_nnet_1)
print(m_nnet_2)
print(m_nnet_3)
print(m_nnet_4)
print(m_nnet_5)

ggplot(m_nnet_1)
ggplot(m_nnet_2)
ggplot(m_nnet_3)
ggplot(m_nnet_4)
ggplot(m_nnet_5)

# Predict neural network success on each test data now:
NNETPredictions.1 <- predict(m_nnet_1, test.data.1)
# Create a confusion matrix:
cmNNET.1 <- confusionMatrix(NNETPredictions.1, test.data.1$utah_score)
print(cmNNET.1)

NNETPredictions.2 <- predict(m_nnet_2, test.data.2)
# Create confusion matrix
cmNNET.2 <- confusionMatrix(NNETPredictions.2, test.data.2$utah_score)
print(cmNNET.2)

NNETPredictions.3 <- predict(m_nnet_3, test.data.3)
# Create confusion matrix
cmNNET.3 <- confusionMatrix(NNETPredictions.3, test.data.3$utah_score)
print(cmNNET.3)

NNETPredictions.4 <- predict(m_nnet_4, test.data.4)
# Create confusion matrix
cmNNET.4 <- confusionMatrix(NNETPredictions.4, test.data.4$utah_score)
print(cmNNET.4)

NNETPredictions.5 <- predict(m_nnet_5, test.data.5)
# Create confusion matrix
cmNNET.5 <- confusionMatrix(NNETPredictions.5, test.data.5$utah_score)
print(cmNNET.5)

## Support Vector Machines 1-5.
set.seed(12345)

m_svm_1 <- caret::train(utah_score~., data=train.data.1, method="svmPoly", metric=metric, 
                      trControl=control, preProcess = c("center", "scale"),na.action=na.omit,trace = TRUE,
                      maxit = 20000)

m_svm_2 <- caret::train(utah_score~., data=train.data.2, method="svmPoly", metric=metric, 
                      trControl=control, preProcess = c("center", "scale"),na.action=na.omit,trace = TRUE,
                      maxit = 20000)

m_svm_3 <- caret::train(utah_score~., data=train.data.3, method="svmPoly", metric=metric, 
                      trControl=control, preProcess = c("center", "scale"),na.action=na.omit,trace = TRUE,
                      maxit = 20000)

m_svm_4 <- caret::train(utah_score~., data=train.data.4, method="svmPoly", metric=metric, 
                      trControl=control, preProcess = c("center", "scale"),na.action=na.omit,trace = TRUE,
                      maxit = 20000)

m_svm_5 <- caret::train(utah_score~., data=train.data.5, method="svmPoly", metric=metric, 
                      trControl=control, preProcess = c("center", "scale"),na.action=na.omit,trace = TRUE,
                      maxit = 20000)
print(m_svm_1)
print(m_svm_2)
print(m_svm_3)
print(m_svm_4) 
print(m_svm_5)

ggplot(m_svm_1)
ggplot(m_svm_2)
ggplot(m_svm_3)
ggplot(m_svm_4)
ggplot(m_svm_5)

  # Predict SVM success on test data now:
SVMPredictions.1 <- predict(m_svm_1, test.data.1)
# Create a confusion matrix:
cmSVM.1 <-confusionMatrix(SVMPredictions.1, test.data.1$utah_score)
print(cmSVM.1)

SVMPredictions.2 <- predict(m_svm_2, test.data.2)
# Create confusion matrix
cmSVM.2 <-confusionMatrix(SVMPredictions.2, test.data.2$utah_score)
print(cmSVM.2)

SVMPredictions.3 <- predict(m_svm_3, test.data.3)
# Create confusion matrix
cmSVM.3 <-confusionMatrix(SVMPredictions.3, test.data.3$utah_score)
print(cmSVM.3)

SVMPredictions.4 <- predict(m_svm_4, test.data.4)
# Create confusion matrix
cmSVM.4 <-confusionMatrix(SVMPredictions.4, test.data.4$utah_score)
print(cmSVM.4)

SVMPredictions.5 <- predict(m_svm_5, test.data.5)
# Create confusion matrix
cmSVM.5 <-confusionMatrix(SVMPredictions.5, test.data.5$utah_score)
print(cmSVM.5)

## Summarize all the model training results:
boosting_results.1 <- resamples(list(m_nnet=m_nnet_1, svm=m_svm_1))
summary(boosting_results.1)
dotplot(boosting_results.1)

boosting_results.2 <- resamples(list(m_nnet=m_nnet_2, svm=m_svm_2))
summary(boosting_results.2)
dotplot(boosting_results.2)

boosting_results.3 <- resamples(list(m_nnet=m_nnet_3, svm=m_svm_3))
summary(boosting_results.3)
dotplot(boosting_results.3)

boosting_results.4 <- resamples(list(m_nnet=m_nnet_4, svm=m_svm_4))
summary(boosting_results.4)
dotplot(boosting_results.4)

boosting_results.5 <- resamples(list(m_nnet=m_nnet_5, svm=m_svm_5))
summary(boosting_results.5)
dotplot(boosting_results.5)

## Random forests 1-5
# We're doing this ML algorithm with a search = "random" control parameter.
set.seed(12345)

# Number randomely variable selected is mtry
mtry <- sqrt(ncol(train.data.1))
tunegrid <- expand.grid(.mtry=mtry)
control <- caret::trainControl(method="cv", number=10, classProbs= TRUE,summaryFunction = defaultSummary,
                               selectionFunction = "best",search="random")
rf_1 <- caret::train(utah_score~., 
                    data=train.data.1, 
                    method='rf', 
                    metric=metric,
                    tuneGrid = tunegrid,
                    trControl=control)
print(rf_1)

mtry <- sqrt(ncol(train.data.2))
tunegrid <- expand.grid(.mtry=mtry)
rf_2 <- caret::train(utah_score~., 
                     data=train.data.2, 
                     method='rf', 
                     metric=metric,
                     tuneGrid = tunegrid,
                     trControl=control)
print(rf_2)

mtry <- sqrt(ncol(train.data.3))
tunegrid <- expand.grid(.mtry=mtry)
rf_3 <- caret::train(utah_score~., 
                     data=train.data.3, 
                     method='rf', 
                     metric=metric,
                     tuneGrid = tunegrid,
                     trControl=control)
print(rf_3)

mtry <- sqrt(ncol(train.data.4))
tunegrid <- expand.grid(.mtry=mtry)
rf_4 <- caret::train(utah_score~., 
                     data=train.data.4, 
                     method='rf', 
                     metric=metric,
                     tuneGrid = tunegrid,
                     trControl=control)
print(rf_4)

mtry <- sqrt(ncol(train.data.5))
tunegrid <- expand.grid(.mtry=mtry)
rf_5 <- caret::train(utah_score~., 
                     data=train.data.5, 
                     method='rf', 
                     metric=metric,
                     tuneGrid = tunegrid,
                     trControl=control)
print(rf_5)


## So it appears, our data sucks for predicting utah_score, but random forest was the clear winner.
## But that was fun for you, I hope?
